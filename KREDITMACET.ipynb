{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b12d8a",
   "metadata": {},
   "source": [
    "# Model Prediksi Kredit Macet\n",
    "## Analisis Perbandingan Model dengan Teknik Oversampling\n",
    "\n",
    "Notebook ini berisi analisis komprehensif untuk memprediksi kredit macet menggunakan berbagai algoritma machine learning dan teknik oversampling.\n",
    "\n",
    "### Target:\n",
    "- **KETERANGAN**: 5 label (Lancar, Dalam Perhatian Khusus, Kurang Lancar, Diragukan, Macet)\n",
    "\n",
    "### Metodologi:\n",
    "1. **Data Preprocessing**: Pembersihan data dan feature engineering\n",
    "2. **Oversampling**: SMOTE dan ADASYN untuk mengatasi ketidakseimbangan kelas\n",
    "3. **Model Comparison**: Random Forest, XGBoost, LightGBM, Gradient Boosting\n",
    "4. **Hyperparameter Tuning**: Optimasi model terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Imbalanced Learning Libraries\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Python version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79971da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Assuming the data is in CSV format. Adjust the file path as needed.\n",
    "# You can load from CSV, Excel, or create directly from the provided sample data\n",
    "\n",
    "# Create sample dataset based on provided information\n",
    "data = {\n",
    "    'NO': [1, 2, 3, 4, 5, 6],\n",
    "    'PEKERJAAN': [37, 9, 9, 37, 37, 37],\n",
    "    'KETERANGAN_PEKERJAAN': [\n",
    "        'Pegawai pemerintahan/lembaga negara',\n",
    "        'Pengajar (Guru,Dosen)',\n",
    "        'Pengajar (Guru,Dosen)',\n",
    "        'Pegawai pemerintahan/lembaga negara',\n",
    "        'Pegawai pemerintahan/lembaga negara',\n",
    "        'Pegawai pemerintahan/lembaga negara'\n",
    "    ],\n",
    "    'TANGGAL_LAHIR': ['1978-06-06', '1975-04-30', '1986-03-15', '1971-09-12', '1976-09-23', '1981-08-15'],\n",
    "    'STATUS_PERNIKAHAN': ['K', 'B', 'K', 'K', 'K', 'K'],\n",
    "    'KETERANGAN_STATUS_PERNIKAHAN': ['Kawin', 'Belum Kawin', 'Kawin', 'Kawin', 'Kawin', 'Kawin'],\n",
    "    'KELURAHAN': ['GROGOL UTARA', 'JURANGMANGU BARAT', 'PULAU PANGGANG', 'JOGLO', 'KELAPA GADING TIMUR', 'TIRTAJAYA'],\n",
    "    'KECAMATAN': ['KEBAYORAN LAMA', 'PONDOK AREN', 'KEPULAUAN SERIBU UTARA', 'KEMBANGAN', 'KELAPA GADING', 'SUKMAJAYA'],\n",
    "    'KOTA': ['JAKARTA SELATAN', 'TANGERANG SELATAN', 'KEPULAUAN SERIBU', 'JAKARTA BARAT', 'JAKARTA UTARA', 'KOTA DEPOK'],\n",
    "    'PROVINSI': ['DKI JAKARTA', 'BANTEN', 'DKI JAKARTA', 'DKI JAKARTA', 'DKI JAKARTA', 'JAWA BARAT'],\n",
    "    'PRODUK': ['Konsumer', 'Konsumer', 'Konsumer', 'Konsumer', 'Konsumer', 'Konsumer'],\n",
    "    'SUB_PRODUK': ['KMG', 'KMG', 'KMG', 'KMG', 'KMG', 'KPR'],\n",
    "    'KETERANGAN_SUB_PRODUK': ['Kredit Multi Guna', 'Kredit Multi Guna', 'Kredit Multi Guna', 'Kredit Multi Guna', 'Kredit Multi Guna', 'Kredit Pemilikan Rumah'],\n",
    "    'TANGGAL_INPUT': ['2022-10-03', '2024-09-27', '2024-06-03', '2020-07-07', '2024-04-24', '2015-02-24'],\n",
    "    'PLAFOND': [141200000.00, 418000000.00, 265500000.00, 305000000.00, 91000000.00, 177100000.00],\n",
    "    'JK_WAKTUBULAN': [60, 126, 192, 109, 60, 161],\n",
    "    'STATUS_PRESCREENING': ['Low', 'High', 'Low', 'Low', 'High', 'Medium'],\n",
    "    'KOLEKTABILITAS': [1, 5, 1, 4, 3, 2],\n",
    "    'KETERANGAN': ['Lancar', 'Macet', 'Lancar', 'Diragukan', 'Kurang Lancar', 'Dalam Perhatian Khusus']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# If you have a CSV file, uncomment and use this instead:\n",
    "# df = pd.read_csv('path_to_your_file.csv')\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029743c",
   "metadata": {},
   "source": [
    "## 1. Data Exploration dan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Information about Dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nColumn Types:\")\n",
    "print(\"=\"*50)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(\"=\"*50)\n",
    "print(df['KETERANGAN'].value_counts())\n",
    "print(\"\\nTarget Proportions:\")\n",
    "print(df['KETERANGAN'].value_counts(normalize=True).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Data Preprocessing\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create new features and preprocess existing ones\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df_processed['TANGGAL_LAHIR'] = pd.to_datetime(df_processed['TANGGAL_LAHIR'])\n",
    "    df_processed['TANGGAL_INPUT'] = pd.to_datetime(df_processed['TANGGAL_INPUT'])\n",
    "    \n",
    "    # Calculate age at time of application\n",
    "    df_processed['UMUR'] = (df_processed['TANGGAL_INPUT'] - df_processed['TANGGAL_LAHIR']).dt.days / 365.25\n",
    "    \n",
    "    # Create age groups\n",
    "    df_processed['KELOMPOK_UMUR'] = pd.cut(df_processed['UMUR'], \n",
    "                                          bins=[0, 30, 40, 50, 60, 100], \n",
    "                                          labels=['<30', '30-40', '40-50', '50-60', '60+'])\n",
    "    \n",
    "    # Create plafond groups (loan amount categories)\n",
    "    df_processed['KATEGORI_PLAFOND'] = pd.cut(df_processed['PLAFOND'], \n",
    "                                             bins=[0, 100000000, 200000000, 300000000, float('inf')], \n",
    "                                             labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # Create loan term categories\n",
    "    df_processed['KATEGORI_TENOR'] = pd.cut(df_processed['JK_WAKTUBULAN'], \n",
    "                                           bins=[0, 60, 120, 180, float('inf')], \n",
    "                                           labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "    \n",
    "    # Risk score based on prescreening status\n",
    "    risk_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "    df_processed['RISK_SCORE'] = df_processed['STATUS_PRESCREENING'].map(risk_mapping)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply feature engineering\n",
    "df_processed = create_features(df)\n",
    "\n",
    "print(\"New Features Created:\")\n",
    "print(\"=\"*50)\n",
    "new_features = ['UMUR', 'KELOMPOK_UMUR', 'KATEGORI_PLAFOND', 'KATEGORI_TENOR', 'RISK_SCORE']\n",
    "for feature in new_features:\n",
    "    print(f\"{feature}: {df_processed[feature].nunique()} unique values\")\n",
    "    if df_processed[feature].dtype == 'object' or df_processed[feature].dtype.name == 'category':\n",
    "        print(f\"  Values: {df_processed[feature].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(f\"  Range: {df_processed[feature].min():.2f} - {df_processed[feature].max():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "def prepare_model_data(df):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning models\n",
    "    \"\"\"\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Select relevant features for modeling\n",
    "    feature_columns = [\n",
    "        'PEKERJAAN', 'STATUS_PERNIKAHAN', 'PROVINSI', 'SUB_PRODUK',\n",
    "        'PLAFOND', 'JK_WAKTUBULAN', 'STATUS_PRESCREENING', 'KOLEKTABILITAS',\n",
    "        'UMUR', 'RISK_SCORE'\n",
    "    ]\n",
    "    \n",
    "    # Add categorical features that need encoding\n",
    "    categorical_features = [\n",
    "        'STATUS_PERNIKAHAN', 'PROVINSI', 'SUB_PRODUK', 'STATUS_PRESCREENING'\n",
    "    ]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for feature in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df_model[f'{feature}_encoded'] = le.fit_transform(df_model[feature])\n",
    "        label_encoders[feature] = le\n",
    "    \n",
    "    # Final feature set\n",
    "    final_features = [\n",
    "        'PEKERJAAN', 'STATUS_PERNIKAHAN_encoded', 'PROVINSI_encoded', 'SUB_PRODUK_encoded',\n",
    "        'PLAFOND', 'JK_WAKTUBULAN', 'STATUS_PRESCREENING_encoded', 'KOLEKTABILITAS',\n",
    "        'UMUR', 'RISK_SCORE'\n",
    "    ]\n",
    "    \n",
    "    X = df_model[final_features]\n",
    "    y = df_model['KETERANGAN']\n",
    "    \n",
    "    return X, y, label_encoders, final_features\n",
    "\n",
    "# Prepare data\n",
    "X, y, label_encoders, feature_names = prepare_model_data(df_processed)\n",
    "\n",
    "print(\"Model Data Preparation:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"\\nTarget classes: {y.unique()}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Display sample of prepared data\n",
    "print(f\"\\nSample of prepared features:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f1e7d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ea933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Target distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "y.value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Credit Status (KETERANGAN)')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plafond distribution by status\n",
    "plt.subplot(2, 3, 2)\n",
    "df_processed.boxplot(column='PLAFOND', by='KETERANGAN', ax=plt.gca())\n",
    "plt.title('Plafond Distribution by Credit Status')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Age distribution by status\n",
    "plt.subplot(2, 3, 3)\n",
    "df_processed.boxplot(column='UMUR', by='KETERANGAN', ax=plt.gca())\n",
    "plt.title('Age Distribution by Credit Status')\n",
    "plt.suptitle('')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Loan term distribution by status\n",
    "plt.subplot(2, 3, 4)\n",
    "df_processed.boxplot(column='JK_WAKTUBULAN', by='KETERANGAN', ax=plt.gca())\n",
    "plt.title('Loan Term Distribution by Credit Status')\n",
    "plt.suptitle('')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Kolektabilitas distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "df_processed['KOLEKTABILITAS'].value_counts().sort_index().plot(kind='bar', color='lightcoral')\n",
    "plt.title('Kolektabilitas Distribution')\n",
    "plt.xlabel('Kolektabilitas')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(2, 3, 6)\n",
    "correlation_features = ['PLAFOND', 'JK_WAKTUBULAN', 'KOLEKTABILITAS', 'UMUR', 'RISK_SCORE']\n",
    "corr_matrix = df_processed[correlation_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=plt.gca())\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Class imbalance analysis\n",
    "print(\"Class Imbalance Analysis:\")\n",
    "print(\"=\"*50)\n",
    "class_counts = y.value_counts()\n",
    "total_samples = len(y)\n",
    "for class_name, count in class_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"{class_name}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nImbalance Ratio (Majority/Minority): {class_counts.max() / class_counts.min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3165f47",
   "metadata": {},
   "source": [
    "## 3. Handling Class Imbalance dengan Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Scale features for oversampling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeatures scaled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply oversampling techniques\n",
    "def apply_oversampling(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Apply SMOTE and ADASYN oversampling techniques\n",
    "    \"\"\"\n",
    "    # Original dataset\n",
    "    original_data = {\n",
    "        'X': X_train,\n",
    "        'y': y_train,\n",
    "        'name': 'Original'\n",
    "    }\n",
    "    \n",
    "    # SMOTE oversampling\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "    smote_data = {\n",
    "        'X': X_smote,\n",
    "        'y': y_smote,\n",
    "        'name': 'SMOTE'\n",
    "    }\n",
    "    \n",
    "    # ADASYN oversampling\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_adasyn, y_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "    adasyn_data = {\n",
    "        'X': X_adasyn,\n",
    "        'y': y_adasyn,\n",
    "        'name': 'ADASYN'\n",
    "    }\n",
    "    \n",
    "    return [original_data, smote_data, adasyn_data]\n",
    "\n",
    "# Apply oversampling\n",
    "datasets = apply_oversampling(X_train_scaled, y_train)\n",
    "\n",
    "# Compare dataset sizes and distributions\n",
    "print(\"Oversampling Results:\")\n",
    "print(\"=\"*70)\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{dataset['name']} Dataset:\")\n",
    "    print(f\"Shape: {dataset['X'].shape}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    class_dist = pd.Series(dataset['y']).value_counts().sort_index()\n",
    "    for class_name, count in class_dist.items():\n",
    "        percentage = (count / len(dataset['y'])) * 100\n",
    "        print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize the impact of oversampling\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    pd.Series(dataset['y']).value_counts().plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'{dataset[\"name\"]} Dataset\\n({len(dataset[\"y\"])} samples)')\n",
    "    plt.xlabel('Credit Status')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7df908",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "def get_models():\n",
    "    \"\"\"\n",
    "    Define the machine learning models to compare\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "        'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1_Macro': f1_macro,\n",
    "        'F1_Weighted': f1_weighted,\n",
    "        'Precision_Macro': precision_macro,\n",
    "        'Recall_Macro': recall_macro,\n",
    "        'CV_F1_Mean': cv_mean,\n",
    "        'CV_F1_Std': cv_std,\n",
    "        'Trained_Model': model,\n",
    "        'Predictions': y_pred\n",
    "    }\n",
    "\n",
    "print(\"Models defined successfully!\")\n",
    "print(\"Models to compare:\", list(get_models().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models on all datasets\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "print(\"Training and Evaluating Models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nEvaluating on {dataset['name']} dataset...\")\n",
    "    \n",
    "    models = get_models()\n",
    "    dataset_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        result = evaluate_model(\n",
    "            model, dataset['X'], X_test_scaled, dataset['y'], y_test, \n",
    "            model_name, dataset['name']\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        dataset_results[model_name] = result\n",
    "        \n",
    "        print(f\"    Accuracy: {result['Accuracy']:.4f}\")\n",
    "        print(f\"    F1-Macro: {result['F1_Macro']:.4f}\")\n",
    "        print(f\"    CV F1-Macro: {result['CV_F1_Mean']:.4f} ± {result['CV_F1_Std']:.4f}\")\n",
    "    \n",
    "    # Find best model for this dataset\n",
    "    best_model_name = max(dataset_results.keys(), \n",
    "                         key=lambda x: dataset_results[x]['F1_Macro'])\n",
    "    best_models[dataset['name']] = {\n",
    "        'name': best_model_name,\n",
    "        'result': dataset_results[best_model_name]\n",
    "    }\n",
    "    print(f\"  Best model for {dataset['name']}: {best_model_name}\")\n",
    "\n",
    "print(\"\\nModel evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results table\n",
    "print(\"Comprehensive Model Comparison Results:\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['Model', 'Dataset', 'Accuracy', 'F1_Macro', 'F1_Weighted', 'CV_F1_Mean']\n",
    "print(results_df[display_cols].round(4))\n",
    "\n",
    "# Find overall best combination\n",
    "best_overall = results_df.loc[results_df['F1_Macro'].idxmax()]\n",
    "print(f\"\\nOverall Best Combination:\")\n",
    "print(f\"Model: {best_overall['Model']}\")\n",
    "print(f\"Dataset: {best_overall['Dataset']}\")\n",
    "print(f\"F1-Macro Score: {best_overall['F1_Macro']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# F1-Macro scores comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "pivot_f1 = results_df.pivot(index='Model', columns='Dataset', values='F1_Macro')\n",
    "sns.heatmap(pivot_f1, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "plt.title('F1-Macro Scores by Model and Dataset')\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "pivot_acc = results_df.pivot(index='Model', columns='Dataset', values='Accuracy')\n",
    "sns.heatmap(pivot_acc, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "plt.title('Accuracy Scores by Model and Dataset')\n",
    "\n",
    "# Cross-validation F1 scores\n",
    "plt.subplot(2, 2, 3)\n",
    "pivot_cv = results_df.pivot(index='Model', columns='Dataset', values='CV_F1_Mean')\n",
    "sns.heatmap(pivot_cv, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "plt.title('Cross-Validation F1-Macro Scores')\n",
    "\n",
    "# Bar plot of best results per dataset\n",
    "plt.subplot(2, 2, 4)\n",
    "best_results = results_df.groupby('Dataset')['F1_Macro'].max()\n",
    "best_results.plot(kind='bar', color='lightblue')\n",
    "plt.title('Best F1-Macro Score per Dataset')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('F1-Macro Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of best models per dataset\n",
    "print(\"\\nBest Models per Dataset:\")\n",
    "print(\"=\"*50)\n",
    "for dataset_name, best_info in best_models.items():\n",
    "    result = best_info['result']\n",
    "    print(f\"{dataset_name} Dataset:\")\n",
    "    print(f\"  Best Model: {best_info['name']}\")\n",
    "    print(f\"  F1-Macro: {result['F1_Macro']:.4f}\")\n",
    "    print(f\"  Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb6130",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning untuk Model Terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model and dataset for hyperparameter tuning\n",
    "best_model_name = best_overall['Model']\n",
    "best_dataset_name = best_overall['Dataset']\n",
    "\n",
    "print(f\"Hyperparameter Tuning for: {best_model_name} on {best_dataset_name} dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the best dataset\n",
    "best_dataset = next(d for d in datasets if d['name'] == best_dataset_name)\n",
    "X_train_best = best_dataset['X']\n",
    "y_train_best = best_dataset['y']\n",
    "\n",
    "# Define hyperparameter grids for different models\n",
    "def get_param_grid(model_name):\n",
    "    \"\"\"\n",
    "    Define hyperparameter grids for different models\n",
    "    \"\"\"\n",
    "    if model_name == 'Random Forest':\n",
    "        return {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif model_name == 'XGBoost':\n",
    "        return {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    elif model_name == 'LightGBM':\n",
    "        return {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'num_leaves': [31, 50, 100]\n",
    "        }\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        return {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Get the model and parameter grid\n",
    "models_dict = get_models()\n",
    "best_model = models_dict[best_model_name]\n",
    "param_grid = get_param_grid(best_model_name)\n",
    "\n",
    "print(f\"Parameter grid for {best_model_name}:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal combinations to try: {np.prod([len(v) for v in param_grid.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search with Cross-Validation\n",
    "print(\"Performing Grid Search...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Use stratified k-fold for cross-validation\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=best_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train_best, y_train_best)\n",
    "\n",
    "# Get results\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best CV F1-Macro score: {best_cv_score:.4f}\")\n",
    "\n",
    "# Compare with original model\n",
    "original_model = models_dict[best_model_name]\n",
    "original_model.fit(X_train_best, y_train_best)\n",
    "original_pred = original_model.predict(X_test_scaled)\n",
    "original_f1 = f1_score(y_test, original_pred, average='macro')\n",
    "\n",
    "# Evaluate tuned model on test set\n",
    "tuned_pred = best_tuned_model.predict(X_test_scaled)\n",
    "tuned_f1 = f1_score(y_test, tuned_pred, average='macro')\n",
    "tuned_accuracy = accuracy_score(y_test, tuned_pred)\n",
    "\n",
    "print(f\"\\nComparison on Test Set:\")\n",
    "print(f\"Original model F1-Macro: {original_f1:.4f}\")\n",
    "print(f\"Tuned model F1-Macro: {tuned_f1:.4f}\")\n",
    "print(f\"Improvement: {tuned_f1 - original_f1:.4f}\")\n",
    "print(f\"Tuned model Accuracy: {tuned_accuracy:.4f}\")\n",
    "\n",
    "# Display top 10 parameter combinations\n",
    "results_df_tuning = pd.DataFrame(grid_search.cv_results_)\n",
    "top_results = results_df_tuning.nlargest(10, 'mean_test_score')[\n",
    "    ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "]\n",
    "\n",
    "print(f\"\\nTop 10 Parameter Combinations:\")\n",
    "print(\"=\"*80)\n",
    "for i, (idx, row) in enumerate(top_results.iterrows()):\n",
    "    print(f\"{i+1}. Score: {row['mean_test_score']:.4f} ± {row['std_test_score']:.4f}\")\n",
    "    print(f\"   Params: {row['params']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e10858",
   "metadata": {},
   "source": [
    "## 6. Final Model Evaluation dan Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bf64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Final Model Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, tuned_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Confusion matrix for tuned model\n",
    "plt.subplot(1, 3, 1)\n",
    "cm_tuned = confusion_matrix(y_test, tuned_pred)\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=y.unique(), yticklabels=y.unique())\n",
    "plt.title(f'Confusion Matrix - Tuned {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Feature importance (if available)\n",
    "plt.subplot(1, 3, 2)\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_tuned_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    sns.barplot(data=feature_importance.head(10), y='feature', x='importance')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "# Model performance comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "comparison_data = {\n",
    "    'Original': original_f1,\n",
    "    'Tuned': tuned_f1\n",
    "}\n",
    "plt.bar(comparison_data.keys(), comparison_data.values(), color=['lightcoral', 'lightgreen'])\n",
    "plt.title('F1-Macro Score Comparison')\n",
    "plt.ylabel('F1-Macro Score')\n",
    "plt.ylim(0, 1)\n",
    "for i, (k, v) in enumerate(comparison_data.items()):\n",
    "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(\"\\nPer-Class Performance Analysis:\")\n",
    "print(\"=\"*60)\n",
    "report_dict = classification_report(y_test, tuned_pred, output_dict=True)\n",
    "classes = [k for k in report_dict.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "\n",
    "per_class_df = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Precision': [report_dict[c]['precision'] for c in classes],\n",
    "    'Recall': [report_dict[c]['recall'] for c in classes],\n",
    "    'F1-Score': [report_dict[c]['f1-score'] for c in classes],\n",
    "    'Support': [report_dict[c]['support'] for c in classes]\n",
    "})\n",
    "\n",
    "print(per_class_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Conclusions\n",
    "print(\"SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"1. DATASET ANALYSIS:\")\n",
    "print(f\"   - Original dataset size: {X.shape[0]} samples\")\n",
    "print(f\"   - Number of features: {X.shape[1]}\")\n",
    "print(f\"   - Target classes: {len(y.unique())} ({', '.join(y.unique())})\")\n",
    "print(f\"   - Class imbalance ratio: {y.value_counts().max() / y.value_counts().min():.2f}\")\n",
    "\n",
    "print(\"\\n2. OVERSAMPLING IMPACT:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"   - {dataset['name']}: {len(dataset['y'])} samples\")\n",
    "\n",
    "print(f\"\\n3. BEST MODEL COMBINATION:\")\n",
    "print(f\"   - Algorithm: {best_model_name}\")\n",
    "print(f\"   - Dataset: {best_dataset_name}\")\n",
    "print(f\"   - Final F1-Macro Score: {tuned_f1:.4f}\")\n",
    "print(f\"   - Final Accuracy: {tuned_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n4. HYPERPARAMETER TUNING IMPACT:\")\n",
    "print(f\"   - Original F1-Macro: {original_f1:.4f}\")\n",
    "print(f\"   - Tuned F1-Macro: {tuned_f1:.4f}\")\n",
    "print(f\"   - Improvement: {tuned_f1 - original_f1:.4f} ({((tuned_f1/original_f1-1)*100):+.1f}%)\")\n",
    "\n",
    "print(f\"\\n5. BEST HYPERPARAMETERS:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(f\"\\n6. MODEL INSIGHTS:\")\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    top_features = feature_importance.head(3)\n",
    "    print(f\"   - Top 3 most important features:\")\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"     * {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n7. RECOMMENDATIONS:\")\n",
    "print(f\"   - Use {best_model_name} with {best_dataset_name} dataset for production\")\n",
    "print(f\"   - Apply hyperparameter tuning to improve performance by {((tuned_f1/original_f1-1)*100):+.1f}%\")\n",
    "if best_dataset_name != 'Original':\n",
    "    print(f\"   - {best_dataset_name} oversampling effectively handles class imbalance\")\n",
    "print(f\"   - Monitor model performance especially for minority classes\")\n",
    "\n",
    "# Save the final model (optional)\n",
    "import joblib\n",
    "model_filename = f\"best_credit_risk_model_{best_model_name.replace(' ', '_').lower()}_{best_dataset_name.lower()}.pkl\"\n",
    "joblib.dump(best_tuned_model, model_filename)\n",
    "print(f\"\\n8. MODEL SAVED:\")\n",
    "print(f\"   - Filename: {model_filename}\")\n",
    "print(f\"   - Model: Tuned {best_model_name} trained on {best_dataset_name} dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba9148",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Rekomendasi\n",
    "\n",
    "### Ringkasan Analisis:\n",
    "1. **Data Preprocessing**: Berhasil membuat fitur-fitur baru seperti umur, kategori plafond, dan risk score\n",
    "2. **Class Imbalance**: Dataset asli mengalami ketidakseimbangan kelas yang signifikan\n",
    "3. **Oversampling**: SMOTE dan ADASYN berhasil mengatasi ketidakseimbangan dengan meningkatkan jumlah sampel minority class\n",
    "4. **Model Comparison**: Membandingkan 4 algoritma (Random Forest, XGBoost, LightGBM, Gradient Boosting) pada 3 dataset\n",
    "5. **Hyperparameter Tuning**: Optimasi model terbaik menghasilkan peningkatan performa\n",
    "\n",
    "### Rekomendasi:\n",
    "- Gunakan kombinasi model dan dataset terbaik yang ditemukan untuk production\n",
    "- Pantau performa model secara berkala, terutama untuk kelas minority\n",
    "- Pertimbangkan mengumpulkan lebih banyak data untuk kelas yang underrepresented\n",
    "- Lakukan re-training model secara berkala dengan data terbaru\n",
    "\n",
    "### Next Steps:\n",
    "1. Implementasi model dalam sistem production\n",
    "2. Setup monitoring dan alerting untuk model drift\n",
    "3. Persiapan pipeline untuk retraining otomatis\n",
    "4. A/B testing untuk validasi performa di real-world scenario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
